{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysis.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shv-om/alohomora-execute22/blob/main/analysis/Notebook/SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Libraries\n",
        "!pip install tweepy\n",
        "!pip install pycountry\n",
        "!pip install textblob\n",
        "!python -m textblob.download_corpora\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBa_i36kveQS",
        "outputId": "efbb5389-cbae-4217-ec70-ba2090034bd3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.7/dist-packages (3.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy) (3.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pycountry\n",
            "  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 5.0 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pycountry) (57.4.0)\n",
            "Building wheels for collected packages: pycountry\n",
            "  Building wheel for pycountry (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681845 sha256=96595616eb8c99c981c5454e4437e13d395ee92ad630ffb54e4afc3c33f79379\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/06/e8/7ee176e95ea9a8a8c3b3afcb1869f20adbd42413d4611c6eb4\n",
            "Successfully built pycountry\n",
            "Installing collected packages: pycountry\n",
            "Successfully installed pycountry-22.3.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Analysis"
      ],
      "metadata": {
        "id": "82eJGqzCv-JR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ih5D58VsVGDO",
        "outputId": "294723bb-0128-4695-afb5-8b0e984636aa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "from textblob import TextBlob\n",
        "import sys\n",
        "import tweepy\n",
        "from tweepy import OAuthHandler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pycountry\n",
        "import re\n",
        "import string\n",
        "import json \n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "XpH6-U79wC2p"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "MFatrCPVVhGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### API Keys"
      ],
      "metadata": {
        "id": "HjpE0xm4Q2XP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## read keys from keys.enc\n",
        "with open('keys.enc', 'r') as keys:\n",
        "  fourKeys = keys.readlines()\n",
        "\n",
        "## load keys\n",
        "GLOBAL_consumer_key = fourKeys[0][:-1]\n",
        "GLOBAL_consumer_secret = fourKeys[1][:-1]\n",
        "GLOBAL_access_token = fourKeys[2][:-1]\n",
        "GLOBAL_access_token_secret = fourKeys[3][:-1]"
      ],
      "metadata": {
        "id": "Tv1EMPg4TEj7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Ak2b8W7WyYMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Supporting Classes"
      ],
      "metadata": {
        "id": "7kGU2VixRes-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class dataCleaner:\n",
        "  def __init__(self):\n",
        "    self.stopword = None\n",
        "    self.ps = None\n",
        "    return\n",
        "  \n",
        "  def remove_punct(self, text):\n",
        "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    text = re.sub(\"[0–9]+\", \"\", text)\n",
        "    return text\n",
        "\n",
        "  def remove_symbols(self, text):\n",
        "    text = re.sub(\"RT @\\w+: \",\"\",text)\n",
        "    text = re.sub(\"[!@,~-]+\",\" \",text)\n",
        "    return text\n",
        "\n",
        "  def tokenization(self, text):\n",
        "    text = re.split('\\W+', text)\n",
        "    return text\n",
        "\n",
        "  def remove_stopwords(self, text, stopword):\n",
        "    text = [word for word in text if word not in stopword]\n",
        "    return text\n",
        "\n",
        "  def stemming(self, text, ps):\n",
        "    text = [ps.stem(word) for word in text]\n",
        "    return text\n",
        "\n",
        "  def basicCleaning(self, tw_list):\n",
        "    #Removing Punctuation\n",
        "    tw_list[\"punct\"] = tw_list[\"text\"].apply(lambda x: self.remove_punct(x))\n",
        "    #Removing Symbols\n",
        "    tw_list[\"symb\"] = tw_list[\"text\"].apply(lambda x: self.remove_symbols(x))\n",
        "    #Appliyng tokenization\n",
        "    tw_list['tokenized'] = tw_list['punct'].apply(lambda x: self.tokenization(x.lower()))\n",
        "    #Removing stopwords\n",
        "    self.stopword = stopword = nltk.corpus.stopwords.words('english')    \n",
        "    tw_list['nonstop'] = tw_list['tokenized'].apply(lambda x: self.remove_stopwords(x, stopword))\n",
        "    #Appliyng Stemmer\n",
        "    self.ps = nltk.PorterStemmer()\n",
        "    tw_list['stemmed'] = tw_list['nonstop'].apply(lambda x: self.stemming(x, self.ps))\n",
        "    #tw_list.head()\n",
        "    return tw_list\n",
        "  \n",
        "  ##########\n",
        "  #Cleaning Text\n",
        "  def clean_text(self, text):\n",
        "    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n",
        "    text_rc = re.sub('[0-9]+', '', text_lc)\n",
        "    tokens = re.split('\\W+', text_rc)    # tokenization\n",
        "    text = [self.ps.stem(word) for word in tokens if word not in self.stopword]  # remove stopwords and stemming\n",
        "    return text\n",
        "  \n",
        "  #Appliyng Countvectorizer\n",
        "  def cVectorizer(self, tw_list):\n",
        "    countVectorizer = CountVectorizer(analyzer=self.clean_text) \n",
        "    countVector = countVectorizer.fit_transform(tw_list[\"text\"])\n",
        "    \n",
        "    count_vect_df = pd.DataFrame(countVector.toarray(), columns=countVectorizer.get_feature_names())\n",
        "    return count_vect_df\n",
        "  \n",
        "  # Most Used Words\n",
        "  def bestTerms(self, df):\n",
        "    count = pd.DataFrame(df.sum())\n",
        "    countdf = count.sort_values(0,ascending=False).head(20)\n",
        "    return countdf\n",
        "  \n",
        "  ################################################\n",
        "\n",
        "  def main(self, df):\n",
        "    tw_list = self.basicCleaning(df)\n",
        "    cvDF = self.cVectorizer(tw_list)\n",
        "    termCountDF = self.bestTerms(cvDF)\n",
        "    return tw_list, termCountDF"
      ],
      "metadata": {
        "id": "wvf1GzqjXqk7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class marketAnalyzer:\n",
        "  def __init__(self):\n",
        "    # keys and tokens from the Twitter Dev Console\n",
        "    consumer_key = GLOBAL_consumer_key\n",
        "    consumer_secret = GLOBAL_consumer_secret\n",
        "    access_token = GLOBAL_access_token\n",
        "    access_token_secret = GLOBAL_access_token_secret\n",
        "    self.api=None\n",
        "    \n",
        "    # attempt authentication\n",
        "    try:\n",
        "      # create OAuthHandler object\n",
        "      auth = OAuthHandler(consumer_key, consumer_secret)\n",
        "      # set access token and secret\n",
        "      auth.set_access_token(access_token, access_token_secret)\n",
        "      # create tweepy API object to fetch tweets\n",
        "      self.api = tweepy.API(auth)\n",
        "    except:\n",
        "      print(\"Error: Authentication Failed\")\n",
        "\n",
        "  def sentimentAnalysis(self, rawData):\n",
        "    tw_list = rawData.copy()\n",
        "    #Calculating Negative, Positive, Neutral and Compound values\n",
        "    tw_list[['polarity', 'subjectivity']] = tw_list['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n",
        "    for index, row in tw_list['text'].iteritems():\n",
        "      score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
        "      neg = score['neg']\n",
        "      neu = score['neu']\n",
        "      pos = score['pos']\n",
        "      comp = score['compound']\n",
        "      if neg > pos:\n",
        "        tw_list.loc[index, 'sentiment'] = \"negative\"\n",
        "      elif pos > neg:\n",
        "        tw_list.loc[index, 'sentiment'] = \"positive\"\n",
        "      else:\n",
        "        tw_list.loc[index, 'sentiment'] = \"neutral\"\n",
        "        tw_list.loc[index, 'neg'] = neg\n",
        "        tw_list.loc[index, 'neu'] = neu\n",
        "        tw_list.loc[index, 'pos'] = pos\n",
        "        tw_list.loc[index, 'compound'] = comp\n",
        "      # return\n",
        "      return tw_list\n",
        "\n",
        "  def count_values_in_column(self, data,feature):\n",
        "    total=data.loc[:,feature].value_counts(dropna=False)\n",
        "    percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n",
        "    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])\n",
        "\n",
        "  #Sentiment Analysis\n",
        "  def percentage(self, part,whole):\n",
        "    return 100 * float(part)/float(whole)\n",
        "\n",
        "  def initCleaning(self, tweet_list):\n",
        "    #Creating new dataframe and new features\n",
        "    tw_list = pd.DataFrame(tweet_list)\n",
        "    tw_list.drop_duplicates(inplace = True)\n",
        "    #Cleaning Text (RT, Punctuation etc)\n",
        "    tw_list[\"text\"] = tw_list[0]\n",
        "    #Removing RT, Punctuation etc\n",
        "    remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n",
        "    tw_list['text'] = tw_list.text.map(remove_rt)\n",
        "    tw_list[\"text\"] = tw_list.text.str.lower()\n",
        "    return tw_list\n",
        "    \n",
        "  def extractAllTweets(self, keyword, noOfTweet: int=200):\n",
        "    #tweets = tweepy.Cursor(api.search, q=keyword).items(noOfTweet)\n",
        "    #tweets =  get_tweets(api, query = keyword, count = noOfTweet)\n",
        "    tweets =  self.api.search(q = keyword, count = noOfTweet, lang='en')\n",
        "    positive, negative, neutral, polarity = 0, 0, 0, 0\n",
        "    tweet_list, neutral_list, negative_list, positive_list = [], [], [], []\n",
        "    for tweet in tweets:\n",
        "      #print(tweet.text)\n",
        "      tweet_list.append(tweet.text)\n",
        "      analysis = TextBlob(tweet.text)\n",
        "      score = SentimentIntensityAnalyzer().polarity_scores(tweet.text)\n",
        "      neg, neu, pos, comp = score['neg'], score['neu'], score['pos'], score['compound']\n",
        "      polarity += analysis.sentiment.polarity\n",
        "\n",
        "      if neg > pos:\n",
        "        negative_list.append(tweet.text)\n",
        "        negative += 1\n",
        "      elif pos > neg:\n",
        "        positive_list.append(tweet.text)\n",
        "        positive += 1\n",
        "    \n",
        "      elif pos == neg:\n",
        "        neutral_list.append(tweet.text)\n",
        "        neutral += 1\n",
        "\n",
        "    positive = self.percentage(positive, noOfTweet)\n",
        "    negative = self.percentage(negative, noOfTweet)\n",
        "    neutral = self.percentage(neutral, noOfTweet)\n",
        "    polarity = self.percentage(polarity, noOfTweet)\n",
        "    positive = format(positive, '.1f')\n",
        "    negative = format(negative, '.1f')\n",
        "    neutral = format(neutral, '.1f')\n",
        "\n",
        "    #Creating PieCart\n",
        "    sentimentCounter = pd.DataFrame({'positive':[positive], 'negative':[negative], 'neutral':[neutral]})\n",
        "    #print(lol.shape)\n",
        "\n",
        "    ######\n",
        "    tweet_list = self.initCleaning(tweet_list)\n",
        "    tweet_list.drop(columns=[0], inplace=True)\n",
        "\n",
        "    return tweet_list, sentimentCounter\n",
        "  \n",
        "  def saveJson(self, jsonDict, fname):\n",
        "    with open(fname, \"w\") as outfile:\n",
        "      json.dump(jsonDict, outfile)\n",
        "    return\n",
        "  \n",
        "  def postProcess(self, df, indexColName, forNa=0):\n",
        "    df.reset_index(inplace=True)\n",
        "    df.rename(columns={'index':indexColName}, inplace=True)\n",
        "    df[indexColName] = df[indexColName].fillna(forNa)\n",
        "    ## dict to json\n",
        "    dictKeys = list(df[df.columns[0]])\n",
        "    dictVals = list(df[df.columns[-1]])\n",
        "    dictionary = dict(zip(dictKeys,dictVals))\n",
        "    return dictionary\n",
        "\n",
        "  ###################################################\n",
        "  ###################################################\n",
        "\n",
        "  def getSentiments(self, query, count=2000):\n",
        "    # get tweets\n",
        "    twDF, sentimentCounter = self.extractAllTweets(query, count)\n",
        "    ##\n",
        "    dc = dataCleaner()\n",
        "    cleanedDF, termCountDF = dc.main(twDF)\n",
        "    ##\n",
        "    sentiDF = self.sentimentAnalysis(cleanedDF)\n",
        "    counter = self.count_values_in_column(sentiDF,\"sentiment\")\n",
        "    ####### SAVING #####\n",
        "    termCountDFDict = self.postProcess(termCountDF,'words',forNa=query)\n",
        "    self.saveJson(termCountDFDict, 'termCountDF.json')\n",
        "    counterFDict = self.postProcess(counter,'sentiment','negative and neutral')\n",
        "    self.saveJson(counterFDict, 'counter.json')\n",
        "    ## jsons\n",
        "    sentimentCounter.to_json('json/sentimentCounter.json')\n",
        "    cleanedDF.to_json('json/cleanedDF.json')\n",
        "    sentiDF.to_json('json/sentiDF.json')\n",
        "    return"
      ],
      "metadata": {
        "id": "szR36MHQyZXx"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n"
      ],
      "metadata": {
        "id": "VqjwMJVWRgiL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Application"
      ],
      "metadata": {
        "id": "DrPdtw_WRj7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MA = marketAnalyzer()\n",
        "MA.getSentiments(query='football', count=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErN2a_mOFg8s",
        "outputId": "316fc275-787e-4cb4-c13b-73c2a2c8b794"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "E-Dqq_aE_RDE"
      }
    }
  ]
}